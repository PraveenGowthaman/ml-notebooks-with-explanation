{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us load the dataset and perform exploratory data analysis",
   "id": "70cd1ef3e21ba062"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from datasets.packaged_modules.pandas.pandas import Pandas\n",
    "\n",
    "train = pd.read_csv(\"playground-series-s5e11/train.csv\")\n",
    "test = pd.read_csv(\"playground-series-s5e11/test.csv\")\n",
    "\n",
    "print(\"Shape of the training data\",train.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us now in further columns look at how data looks\n",
   "id": "2f7892fc015873e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Checking what are different columns\n",
    "train.columns"
   ],
   "id": "1df7f95a8e62affe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Checking for null values in the columns of the training set\n",
    "train.isnull().sum().sort_values(ascending=False)"
   ],
   "id": "39426af3dde4d94a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Checking for null values in the columns of the test set\n",
    "test.isnull().sum().sort_values(ascending=False)"
   ],
   "id": "edd9ffda2177ab8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.info()",
   "id": "e71eb7020c1ca213",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Let us look at the head of the dataframe\n",
    "train.head(10)"
   ],
   "id": "188d8e11505609b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Let us understand how data is distributed\n",
    "train.describe()"
   ],
   "id": "89c1fef4f8346480",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train[\"loan_paid_back\"].value_counts(normalize=True)",
   "id": "70d4fc881c757408",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us gather start modelling",
   "id": "7dc4492b41871d15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove the target and the id column from the train set . Id we have no meaning to keep so we are removing it\n",
    "X = train.drop(columns=[\"id\",\"loan_paid_back\"])\n",
    "y = train[\"loan_paid_back\"]\n",
    "\n",
    "\n",
    "X_test = test.drop(columns=[\"id\"])"
   ],
   "id": "2e79d29092278748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the below step we need to split the data with stratification cause , if you see the above distribution of the loan paid back the classes in that column are not evenly distributed, meaning 1 is around 79 percentage of the column and 0 is around 0.2 . So when scikit learn splits it we might end up with only 1 in the train or split set which would lead to bias",
   "id": "5e0ca7f8bb90bbf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)\n"
   ],
   "id": "ae9824d4683834fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us now split the data , apply some transformations like one hot encoding etc",
   "id": "962c0a9283e6d6fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Just viewing few fields for their skewedness.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "train[\"annual_income\"].hist(bins=50)\n",
    "plt.title(\"Annual Income Distribution\")\n",
    "plt.xlabel(\"Annual Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "train[\"loan_amount\"].hist(bins=50)\n",
    "plt.title(\"Loan Amount Distribution\")\n",
    "plt.xlabel(\"Loan Amount\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Splitting the column names into categorical and numerical columns\n",
    "cat_cols = X_train.select_dtypes(include=\"object\").columns\n",
    "num_cols = X_train.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "#We will not apply any scaling to the numerical features and do one hot encoding to the categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n"
   ],
   "id": "156a8ae6f490a750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let us create a simple logistic regression model and test it\n",
    "\n",
    "we set the class_weight as balanced . Below is a clean explanation from google\n",
    "\n",
    "In sklearn.linear_model.LogisticRegression, the class_weight=\"balanced\" parameter is used to automatically handle imbalanced datasets by adjusting the weights of classes inversely proportional to their frequencies. Here is a breakdown of what it does and how it affects your model: Identifies Imbalance: It automatically calculates the frequencies of your target classes (\\(y\\)) and assigns higher weights to the minority class and lower weights to the majority class.Modifies the Loss Function: Instead of treating every training sample as equally important, class_weight=\"balanced\" penalizes misclassifications of the minority class more heavily than misclassifications of the majority class."
   ],
   "id": "b7f43c98fa80e755"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "553812d97ca48939",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us check the ROC score for our baseline model",
   "id": "49646d2510f4bedb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "val_preds = model.predict_proba(X_val)[:, 1]\n",
    "roc_auc = roc_auc_score(y_val, val_preds)\n",
    "\n",
    "print(\"Validation ROC AUC:\", roc_auc)"
   ],
   "id": "fc814cb85db9dcf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# training on the full dataset\n",
    "\n",
    "model.fit(X, y)\n",
    "test_preds = model.predict_proba(X_test)[:, 1]"
   ],
   "id": "d753567037f288cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Let us sumbit first version to kaggle for scores\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"loan_paid_back\": test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False) # score 0.911"
   ],
   "id": "3175d23a9083ae14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Now let us do Feature Engineering . Create new features that will give more context to the model",
   "id": "ff70f77ccb486c6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "grade_subgrade looks like: C3, D1, F1, etc.\n",
    "\n",
    "This actually contains two pieces of information:\n",
    "\n",
    "Grade: the letter (A, B, C, D, E, F, ...) → broad credit risk bucket\n",
    "\n",
    "Subgrade: the number (1..5 usually) → finer risk level within the grade\n",
    "\n",
    "Why splitting helps\n",
    "\n",
    "If we keep grade_subgrade as a single category, the model learns separate buckets like:\n",
    "\n",
    "“C3 is risky”\n",
    "\n",
    "“C4 is slightly different”\n",
    "…but it doesn’t automatically understand the structure:\n",
    "\n",
    "A < B < C < D < E < F (worse risk)\n",
    "\n",
    "within each grade, 1..5 is also ordered\n",
    "\n",
    "By splitting, we give the model:\n",
    "\n",
    "a coarse signal (grade)\n",
    "\n",
    "a fine signal (subgrade number)\n",
    "\n",
    "This often improves performance because risk is naturally monotonic with these."
   ],
   "id": "de2288e5093be303"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_grade_features(df:pd.DataFrame)->pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"grade\"] = df[\"grade_subgrade\"].str[0]\n",
    "    df[\"subgrade\"] = df[\"grade_subgrade\"].str[1:].astype(int)\n",
    "    return df"
   ],
   "id": "ee6e2e66a1d83c86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Feature A: loan_to_income = loan_amount / annual_income\n",
    "Why this helps\n",
    "\n",
    "A loan of 15,000 is:\n",
    "\n",
    "easy for someone earning 120,000\n",
    "\n",
    "very hard for someone earning 20,000\n",
    "\n",
    "So the model should care more about:\n",
    "\n",
    "“How big is the loan compared to income?”\n",
    "not just the loan amount alone.\n",
    "\n",
    "This feature directly captures affordability."
   ],
   "id": "eb27f7e3cc70fe0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_ratio_features(df:pd.DataFrame)->pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"loan_to_income\"] = df[\"loan_amount\"] / (df[\"annual_income\"] + 1e-6)\n",
    "    #(We add 1e-6 to avoid divide-by-zero, even though income min is > 0 in data.)\n",
    "    return df"
   ],
   "id": "556f587092cf6dff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Feature B: interest_x_dti = interest_rate * debt_to_income_ratio\n",
    "Why this helps\n",
    "\n",
    "High DTI means borrower already has high monthly obligations.\n",
    "\n",
    "High interest rate means the loan is expensive.\n",
    "\n",
    "When both are high, risk compounds. Multiplying captures that “double trouble” effect."
   ],
   "id": "3b9329f22eb7ee9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_interaction_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"interest_x_dti\"] = df[\"interest_rate\"] * df[\"debt_to_income_ratio\"]\n",
    "    return df"
   ],
   "id": "779b54bee2a4fa5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Optional log transformation of the skewed features\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_log_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"log_annual_income\"] = np.log1p(df[\"annual_income\"])\n",
    "    df[\"log_loan_amount\"] = np.log1p(df[\"loan_amount\"])\n",
    "    return df"
   ],
   "id": "7ddd3c13a78ef44f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = add_grade_features(df)\n",
    "    df = add_ratio_features(df)\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_log_features(df)  # optional\n",
    "    return df\n",
    "\n",
    "train_fe = engineer(train)\n",
    "test_fe  = engineer(test)\n"
   ],
   "id": "e64797faed996999",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# We will not use one hot encoding but make the textual columns as category\n",
    "Why NOT one-hot encoding here?\n",
    "\n",
    "One-hot encoding:\n",
    "\n",
    "Explodes dimensionality\n",
    "\n",
    "Slows training\n",
    "\n",
    "Loses ordering/grouping information\n"
   ],
   "id": "43604318a4d14309"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cat_cols = train_fe.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "for col in cat_cols:\n",
    "    train_fe[col] = train_fe[col].astype(\"category\")\n",
    "    test_fe[col] = test_fe[col].astype(\"category\")"
   ],
   "id": "ead307d265fee899",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets next perform cross validation",
   "id": "366a7b3349bc51e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = train_fe.drop(columns=[\"id\", \"loan_paid_back\"])\n",
    "y = train_fe[\"loan_paid_back\"]\n",
    "\n",
    "X_test = test_fe.drop(columns=[\"id\"])\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "78517d7b1a799bcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Why LightGBM\n",
    "\n",
    "LightGBM trains many small trees sequentially.\n",
    "\n",
    "Each new tree:\n",
    "\n",
    "Focuses more on mistakes made by previous trees\n",
    "\n",
    "Improves ranking of difficult samples\n",
    "\n",
    "Optimizes directly for loss related to classification quality\n",
    "\n",
    "This is why boosting usually beats single models on Kaggle.\n",
    "\n",
    "We run RandomizedSearchCV to tune the models hyper parameters to get the best model"
   ],
   "id": "a582d43877192934"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "base_model = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    metric=\"auc\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
    "    \"num_leaves\": [31, 63, 127],\n",
    "    \"max_depth\": [-1, 6, 8, 10],\n",
    "    \"min_child_samples\": [20, 50, 100, 200],\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"n_estimators\": [2000, 4000, 6000]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,                 # increase to 50+ later if you want\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=skf,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X, y)\n",
    "\n",
    "print(\"Best CV AUC:\", search.best_score_)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "\n",
    "best_model = search.best_estimator_"
   ],
   "id": "ca07873c0b15ba04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now take the best model train ti on full dataset and do prediction and get the result pd",
   "id": "c3134b05ecc41d8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T07:17:22.697399Z",
     "start_time": "2026-01-18T07:17:05.738923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model.fit(X, y)\n",
    "\n",
    "test_preds = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_preds"
   ],
   "id": "8a150a46d5f90e14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 474494, number of negative: 119500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2366\n",
      "[LightGBM] [Info] Number of data points in the train set: 593994, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.798820 -> initscore=1.378933\n",
      "[LightGBM] [Info] Start training from score 1.378933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.95853175, 0.9789257 , 0.59697077, ..., 0.98273289, 0.98556905,\n",
       "       0.91681884], shape=(254569,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e00ab456d539d35",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
